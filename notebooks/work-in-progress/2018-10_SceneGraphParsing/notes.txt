
Image Retrieval using Scene Graphs  (2015)
  Justin Johnson; Ranjay Krishna; Michael Stark; 
    Li-Jia Li; David A. Shamma; Michael S. Bernstein; Li Fei-Fei
  https://hci.stanford.edu/publications/2015/scenegraphs/JohnsonCVPR2015.pdf
  https://cs.stanford.edu/people/jcjohns/cvpr15_supp/

Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations  (2016)
  Ranjay Krishna; Yuke Zhu; Oliver Groth; Justin Johnson; Kenji Hata; Joshua Kravitz; Stephanie Chen; Yannis Kalantidis; 
    Li-Jia Li; David A. Shamma; Michael S. Bernstein; Li Fei-Fei
  Ranjay Krishna et al 2016
  https://arxiv.org/abs/1602.07332
  
Scene Graph Parsing as Dependency Parsing
  https://arxiv.org/pdf/1803.09189.pdf
  https://github.com/Yusics/bist-parser/tree/sgparser
  https://yusics.github.io/#about
  

Multiple objective XYZ?
  Structure SVM ?
    1_x_MultitaskStructuredPredictionForEntityAnalysis_ma17a.pdf


PyTorch graph NNs (or DeepMind TF/sonnet library?)
  Graph Infomax?
  https://www.reddit.com/r/MachineLearning/comments/9pbf4p/r_deepmind_is_releasing_their_graph_nn_library/
    https://github.com/deepmind/graph_nets
    https://github.com/rusty1s/pytorch_geometric
    https://github.com/stellargraph/stellargraph

-----------

maincard_10937 :: Visually grounded interaction and language
  https://nips2018vigil.github.io/
  https://cmt3.research.microsoft.com/VIGIL2018
  November 1st 2018 (Final Decisions : November 8th, 2018, first 20 accepted workshop papers only to get tickets - 1 each)
    ** Has potential (ideas stage)


*  Learning to match sentences with graphs
   +  Better groundtruth annotator for "Scene Graph Parsing as Dependency Parsing"
      -  Current Oracle gets only 70% F1 on the actual data
      -  Current best dependency-tree sentence parser idea -> graph gets 50%
      -  So bigger win may be in refining the annotator
      @inproceedings{wang2018sgparser,     
        title={Scene Graph Parsing as Dependency Parsing},  
        author={Wang, Yu-Siang and Liu, Chenxi and Zeng, Xiaohui and Yuille, Alan},  
        booktitle={NAACL},  
        year={2018}
      } 
      
-----------


cd ../2018-10_SceneGraphParsing
#git clone https://github.com/Yusics/bist-parser.git
git clone https://github.com/mdda/bist-parser.git
cd bist-parser
git branch -a  # list the branches
git checkout sgparser
git branch

#Split Visual Genome image_data, all_region_graphs, all_attributes into 10 pieces, 
#  and named these files x_%num, where x={image_data, all_region_graphs, all_attributes} and num={0..9}. 
#  (The size for the every first 9 pieces is all_region_graph.size()/10) and put them into preprocess/. 
#  The reason to split to 10 pieces is for acclerating the preprocessing speed.

# NB : version 1.4 has 'ground truth' synsets for objects and relationships and is "Cleaner"
#   Which versions did the sgparser paper use?

# https://visualgenome.org/  # Using version 1.2 (2016-08-29)
mkdir -p data/visual-genome
cd data/visual-genome

wget https://visualgenome.org/static/data/dataset/image_data.json.zip     #   1.7Mb
wget https://visualgenome.org/static/data/dataset/region_graphs.json.zip  # 316Mb
wget https://visualgenome.org/static/data/dataset/attributes.json.zip     #  79Mb

unzip image_data.json.zip     #   17Mb expanded
unzip region_graphs.json.zip  # 2783Mb expanded 
unzip attributes.json.zip     #  462Mb expanded

# dnf install jq
# Length of files is : 108077, so : Not divisible by 10...


### This is the way mandated by the original bist-parser code : Extremely inefficient

# jq '.[0:2]' attributes.json # seems to work as expected...
# jq '.[0:10800]' attributes.json --compact-output > all_attributes_0.json

for i in 0 1 2 3 4 5 6 7 8 9; do 
  let idx0=i\*10810
  let idx1=(i+1)\*10810
  echo "Processing section ${i} : .[${idx0}:${idx1}]" 
  #jq ".[${idx0}:${idx1}]" image_data.json --compact-output > image_data_${i}.json
  jq ".[${idx0}:${idx1}] | length" image_data.json
  #jq ".[${idx0}:${idx1}]" attributes.json --compact-output > all_attributes_${i}.json
  jq ".[${idx0}:${idx1}]" region_graphs.json --compact-output > all_region_graphs_${i}.json
done

ls -l # 
#-rw-rw-r--. 1 andrewsm andrewsm   83280561 Aug 31  2016 attributes.json.zip
#-rw-r--r--. 1 andrewsm andrewsm  462562682 Aug 30  2016 attributes.json
#-rw-rw-r--. 1 andrewsm andrewsm   36569979 Oct 22 23:27 all_attributes_0.json
#-rw-rw-r--. 1 andrewsm andrewsm   33547444 Oct 22 23:27 all_attributes_1.json
#-rw-rw-r--. 1 andrewsm andrewsm   28669514 Oct 22 23:28 all_attributes_2.json
#-rw-rw-r--. 1 andrewsm andrewsm   29871897 Oct 22 23:28 all_attributes_3.json
#-rw-rw-r--. 1 andrewsm andrewsm   46276399 Oct 22 23:28 all_attributes_4.json
#-rw-rw-r--. 1 andrewsm andrewsm   49162434 Oct 22 23:28 all_attributes_5.json
#-rw-rw-r--. 1 andrewsm andrewsm   49715384 Oct 22 23:29 all_attributes_6.json
#-rw-rw-r--. 1 andrewsm andrewsm   33832425 Oct 22 23:29 all_attributes_7.json
#-rw-rw-r--. 1 andrewsm andrewsm   28747220 Oct 22 23:29 all_attributes_8.json
#-rw-rw-r--. 1 andrewsm andrewsm   28822199 Oct 22 23:29 all_attributes_9.json
#-rw-rw-r--. 1 andrewsm andrewsm  331673448 Sep  1  2016 region_graphs.json.zip
#-rw-r--r--. 1 andrewsm andrewsm 2783274985 Aug 31  2016 region_graphs.json
#-rw-rw-r--. 1 andrewsm andrewsm  268183906 Oct 22 23:41 all_region_graphs_0.json
#-rw-rw-r--. 1 andrewsm andrewsm  267817332 Oct 22 23:43 all_region_graphs_1.json
#-rw-rw-r--. 1 andrewsm andrewsm  235354414 Oct 22 23:44 all_region_graphs_2.json
#-rw-rw-r--. 1 andrewsm andrewsm  240178442 Oct 22 23:46 all_region_graphs_3.json
#-rw-rw-r--. 1 andrewsm andrewsm  249988322 Oct 22 23:47 all_region_graphs_4.json
#-rw-rw-r--. 1 andrewsm andrewsm  252083492 Oct 22 23:48 all_region_graphs_5.json
#-rw-rw-r--. 1 andrewsm andrewsm  251913454 Oct 22 23:50 all_region_graphs_6.json
#-rw-rw-r--. 1 andrewsm andrewsm  267635875 Oct 22 23:51 all_region_graphs_7.json
#-rw-rw-r--. 1 andrewsm andrewsm  236921148 Oct 22 23:52 all_region_graphs_8.json
#-rw-rw-r--. 1 andrewsm andrewsm  238587733 Oct 22 23:54 all_region_graphs_9.json
#-rw-rw-r--. 1 andrewsm andrewsm    1780854 Jun 22  2016 image_data.json.zip
#-rw-r--r--. 1 andrewsm andrewsm   17612822 Jun 21  2016 image_data.json
#-rw-rw-r--. 1 andrewsm andrewsm    1574689 Oct 22 23:32 image_data_0.json
#-rw-rw-r--. 1 andrewsm andrewsm    1651645 Oct 22 23:32 image_data_1.json
#-rw-rw-r--. 1 andrewsm andrewsm    1651530 Oct 22 23:32 image_data_2.json
#-rw-rw-r--. 1 andrewsm andrewsm    1651465 Oct 22 23:32 image_data_3.json
#-rw-rw-r--. 1 andrewsm andrewsm    1630257 Oct 22 23:32 image_data_4.json
#-rw-rw-r--. 1 andrewsm andrewsm    1629786 Oct 22 23:32 image_data_5.json
#-rw-rw-r--. 1 andrewsm andrewsm    1629908 Oct 22 23:32 image_data_6.json
#-rw-rw-r--. 1 andrewsm andrewsm    1651695 Oct 22 23:32 image_data_7.json
#-rw-rw-r--. 1 andrewsm andrewsm    1651518 Oct 22 23:32 image_data_8.json
#-rw-rw-r--. 1 andrewsm andrewsm    1647935 Oct 22 23:32 image_data_9.json

## Let's convert the files into 1 row of json per object x 108077 rows.
# https://github.com/stedolan/jq/wiki/FAQ#streaming-json-parser


cat image_data.json    | jq -n --compact-output --stream 'fromstream(1|truncate_stream(inputs))' > image_data.json.rows
cat attributes.json    | jq -n --compact-output --stream 'fromstream(1|truncate_stream(inputs))' > attributes.json.rows
cat region_graphs.json | jq -n --compact-output --stream 'fromstream(1|truncate_stream(inputs))' > region_graphs.json.rows


cd ../../bist-parser/preprocess/
ln -s ../../data/visual-genome data

. ~/env3/bin/activate   # Preprocessing code updated to be 2/3 compatible

# This just makes sure all the images have ids
bash ./preprocess.sh

# This creates the train/dev split, according to whether the image id is in the pre-computed pickle
python split_preprocess.py ALL coco_train
python split_preprocess.py ALL coco_dev

# Now do the assignments (the "Oracle" for the original paper)

# Takes <4mins on   547,801 sentences
python data_to_conll.py --input ./output/pre_coco_dev.json.rows   --output ./output/coco_dev.conll
cp ./output/coco_dev.conll ./output/coco_dev.conll.oracle

# Takes <6mins on 1,070,158 sentences (--train skips some potentially wierd examples)
python data_to_conll.py --input ./output/pre_coco_train.json.rows --output ./output/coco_train.conll --train # For training a network

# Takes <6mins on 1,070,158 sentences (omitting --train evaluates all sentences)
python data_to_conll.py --input ./output/pre_coco_train.json.rows --output ./output/coco_train.conll.oracle  # Not --train for evaluation

# Test the Oracle performance 

cd ../model/src/utils/evaluation_script/

. ~/env3/bin/activate   # Evaluation code updated to be 2/3 compatible
bash eval.sh  # -> python spice_eval.py

# On the dev set : 
# Predictions count  : 547802 == Ground-truth count : 547802
# SPICE score: 0.6630

# On the train set : 
# Number of predictions :  1070159
# SPICE score: 0.6635


# Chase...  -- Yes, actually bad in the source dataset...
image_id=498263, region_id=5969774, Phrase:  ['toiletin a bathroom']
REFERENCE tuples:	 [['bathroom'], ['toilet'], ('toilet', 'is white'), ('toilet', 'in', 'bathroom')]
PREDICT tuples  :	 [['bathroom']]
spice_score:  0.4


..


# May be no need to change line 10, if the following doesn't error...
# python
# from nltk.corpus import wordnet


TODO first : 

Indices into rows:
  Update region-ifier == split_preprocess  # DONE
  region-to-conll     == data_to_conll     # DONE
  spice test          == spice_eval        # DONE

Look at code : (DONE)
  Do extra Attributes in gold tuples affect the spice score?
    Function seems to be 'evaluate_spice' - which appears not to dump unused tuples


#  Look at image0 in training set :
image0.regions.txt :  "image_id": 2415075
grep 2415075 data/image_data.json.rows 
# --> https://cs.stanford.edu/people/rak248/VG_100K_2/2415075.jpg

# region_graphs :
"relationships":[{ ...
"relationships":[] // nada

wc output/pre_coco_train.json.rows.orig # 1,070,159 rows
wc data/image_data.json.rows            #   108,077 rows

## Hmm - need to check paper to see whether numbers make sense


# This is now in "visual-genome-api-playground.ipynb"
#mkdir -p api; cd api
#git clone https://github.com/ranjaykrishna/visual_genome_python_driver.git
#cd ..
# Have a look at the results back from the original source using : visual-genome-api-playground.ipynb

##   Actually, they seem to check out vs API results, so we just have to accept that the 'genome' isn't very clean...



# Plan(?)

* Create an embedding-based 'reverse Oracle'
  - Takes existing region tuples and caption (like existing Oracle)
  - Goes through the tuples, and finds :
    - (Logic is : every object must be mentioned in caption)
      - each object based on exact word-sequence match
      - each unfound object, based on exact word-sequence match with its wordnet synnet
      - each unfound object, based on embedding similarity
      - must find something
      - where found word is not in  wordnet synnet, add to a 'generalisation substitutions' dictionary
    - (Logic is : every relationship must be mentioned in caption)
      - ditto
    - (Potential Logic is : every ?attribute? must be mentioned in caption)
      - Quite possible that only some attributes are in the caption
      - ?ditto? but going through the above steps for a list of the attribute words, each in turn
      - So attribute words are 'claimed' by the caption against successively more generous criteria
      - But have a minimum cosine similarity limit, whereby similarity is too weak to 'claim'
  - Output is a set of 'verified' tuples, that use the found words in the caption exactly

* More sophisticated Oracle
  - Remove tuples where attributes are not in caption
  - Should improve overall F1 (though seem a slightly unfair comparison)

* Actually, the above is simplistic, since the tuples may contain a lot of irrelevant data already
  -  What can we trust?
     -  Looks like only the phrase is the 'real deal'
     -  We do know that the number of objects+attributes+relationships mentioned 
        can be no more than the number of 'real words' in the phrase
  -  Suggestion (process in order):
     -  Look for all exact single and multi-word matches in the object list
     -  Look for all exact single and multi-word matches in the relationship list for the 'relation word(s)'
     -  Look for all exact single and multi-word matches in the attribute list
     -  #Having eliminated the found stuff, do the same with their synsets
     -  Now do a fuzzy search (based on GloVe?) - giving us cosine matches (averaged across words) for all
     -  Each entity can only have one source word - so match them off
     -  Each 'relationship word' can only have one source word - so match them off
     -  Each attribute can only have one source word - so match them off
     
  -  But also have the constraints 
     -  relationships must join valid objects
     -  attributes must be for valid objects
     
pip install h5py ftfy spacy
python -m spacy download en
  
python tidy_refs.py --multiwords
python tidy_refs.py --testnodes   # No response==Good


#ln -s /mnt/rdai/reddragon/2018-10_ZeroShotRelationships_orig orig
ln -s ../2018-10_ZeroShotRelationships/orig .
ln -s ../2018-10_ZeroShotRelationships/text_utils.py .


python conll_to_transformer.py --phase=train --save_bpe --stub=v128
python conll_to_transformer.py --phase=train --save_bpe --stub=v32 --n_ctx=32


## train = 100mins :

python conll_to_transformer.py --phase=train --save_bpe --stub=v32 --n_ctx=32

#Line 6400000, valid_count=1044916
#Line 6,500,000, valid_count=1,061,160
#Last Line = 6,552,437

Saved data to ./bist-parser/preprocess/output/coco_train.conll_all.hdf5
  bpe_maximum 98, bpe_truncate_count 18, bpe_max 32
Saved bpe data to ./bist-parser/preprocess/output/coco_train.conll_all.bpe
--token_clf=40480
--vocab_count=40481
--tokens_special=3
Wed Oct 31 16:56:10 +08 2018



## dev  = 30mins : 

python conll_to_transformer.py --phase=dev --save_bpe --stub=v32 --n_ctx=32

#Line 3200000, valid_count=522438
#Line 3,300,000, valid_count=538,226
#Last Line = 3,358,266

Saved data to ./bist-parser/preprocess/output/coco_dev.conll_all.hdf5
  bpe_maximum 52, bpe_truncate_count 13, bpe_max 32
Saved bpe data to ./bist-parser/preprocess/output/coco_dev.conll_all.bpe
--token_clf=40480
--vocab_count=40481
--tokens_special=3


python train_grapher.py --stub=v32 --batch_size_per_gpu=128

export INSTANCE_NAME="rdai-tts-p100-vm"
gcloud compute instances start $INSTANCE_NAME

gcloud compute ssh $INSTANCE_NAME

cd ~/deep-learning-workshop/notebooks/work-in-progress/2018-10_SceneGraphParsing
mkdir -p checkpoints
sudo mount -o discard,defaults /dev/sdb checkpoints
sudo chmod a+x checkpoints


#gcloud compute scp $INSTANCE_NAME:[REMOTE_FILE_PATH]
gcloud compute scp bist-parser/preprocess/output/*_v32* rdai-tts-p100-vm:~/deep-learning-workshop/notebooks/work-in-progress/2018-10_SceneGraphParsing/bist-parser/preprocess/output/

# Rerun with label==0 change
python train_grapher.py --stub=v32 --batch_size_per_gpu=128 --relation_hdf5=coco_train.conll_v32.hdf5

# Previous with deps==0 (seems wrong)
python train_grapher.py --stub=v32 --batch_size_per_gpu=128 --relation_hdf5=coco_dev.conll_v32.hdf5 --predict --checkpoint=checkpoints/model-grapher_v32_02-0354304.pth

# Rerun with label==0 change
python train_grapher.py --stub=v32label --batch_size_per_gpu=128 --relation_hdf5=coco_dev.conll_v32.hdf5 --predict --checkpoint=./checkpoints/model-grapher_v32_03-0615424.pth


gcloud compute scp rdai-tts-p100-vm:~/deep-learning-workshop/notebooks/work-in-progress/2018-10_SceneGraphParsing/bist-parser/preprocess/output/*_v32.npz bist-parser/preprocess/output/ 
python transformer_to_conll.py # defaults should be fine

gcloud compute scp rdai-tts-p100-vm:~/deep-learning-workshop/notebooks/work-in-progress/2018-10_SceneGraphParsing/bist-parser/preprocess/output/coco_dev.conll_v32.hdf5_v32label.npz bist-parser/preprocess/output/ 
python transformer_to_conll.py --npz=coco_dev.conll_v32.hdf5_v32label.npz --conll=coco_dev.conll_v32label


cd bist-parser/model/src/utils/evaluation_script/

. ~/env3/bin/activate   # Evaluation code updated to be 2/3 compatible
bash eval.sh  # -> python spice_eval.py

preproc=../../../../preprocess/output

# Evaluate Oracle : 
time python spice_eval.py ${preproc}/pre_coco_dev.json.rows ${preproc}/coco_dev.conll.oracle                  # SPICE score: 0.6630
time python spice_eval.py ${preproc}/pre_coco_dev.json.rows ${preproc}/coco_dev.conll.oracle --limit_tuples   # SPICE score: 0.7256


image_id=150358, region_id=21789, Phrase:  ['blue and red bus']
REFERENCE tuples:	 [['bus'], ('bus', 'red'), ('bus', 'passenger'), ('bus', 'black'), ('bus', 'white')]
1	blue	4	ATTR	ATTR
2	and	_	_	_
3	red	4	ATTR	ATTR
4	bus	0	_	OBJ
PREDICT tuples  :	 [['bus'], ('bus', 'red'), ('bus', 'blue')]
Max words in :  {'bus', 'red', 'blue'}
count_tuple=3, num_pred=3, num_ref=5 -- tuples_max=3
num_ref=tuples_max
spice_score:  1.0



# Evaluate Transformer : 
time python spice_eval.py ${preproc}/pre_coco_dev.json.rows ${preproc}/coco_dev.conll_v32label                  # SPICE score: 0.5221
time python spice_eval.py ${preproc}/pre_coco_dev.json.rows ${preproc}/coco_dev.conll_v32label --limit_tuples   # SPICE score: 0.5750



* Transformer-based parser of the original captions
  - Add directed arrow 'head' which does attention-tracking (like dependency tree idea for ZeroShotRelationships)
    - Attributes point to Subject / Object
    - Subject points to relationship
    - Object points to relationship (like a dependency parse)
    - multi-word elements point to previous word in phrase
  - Add word-type 
    - 0=Irrelevant word
    - 1=Attribute
    - 2=Subject
    - 3=Object
    - 4=Relationship
  - Training should be done in 'regular way' 
    - constructing the targets from the caption+'verified tuples' (which now match exactly)
  - For test/predictions, reconstruct graph by looking at the output attention+types
    - Also include 'translations' from the 'generalisation substitutions' dictionary





https://visualgenome.org/VGViz/explore
https://visualgenome.org/api/v0/api_beginners_tutorial.html
https://visualgenome.org/api/v0/api_home.html
https://visualgenome.org/static/data/dataset/readme_v1_4.txt




images.forEach(i=>console.log(i.image_id,i.url,i.regions[0].text));  // white fluffy cat page
VM1951:1 2405715 "https://cs.stanford.edu/people/rak248/VG_100K_2/2405715.jpg" ["Stone wall the cat is sitting on"]
VM1951:1 2358832 "https://cs.stanford.edu/people/rak248/VG_100K/2358832.jpg" ["a cat sitting down"]
VM1951:1 2351438 "https://cs.stanford.edu/people/rak248/VG_100K/2351438.jpg" ["a cat with a backpack"]
VM1951:1 2387285 "https://cs.stanford.edu/people/rak248/VG_100K_2/2387285.jpg" ["cat licking itself"]
VM1951:1 2412335 "https://cs.stanford.edu/people/rak248/VG_100K_2/2412335.jpg" ["A cat is sitting on a bag."]
VM1951:1 2335967 "https://cs.stanford.edu/people/rak248/VG_100K/2335967.jpg" ["A white, orange and black cat"]
VM1951:1 2400035 "https://cs.stanford.edu/people/rak248/VG_100K_2/2400035.jpg" ["a tabby cat"]
VM1951:1 2393745 "https://cs.stanford.edu/people/rak248/VG_100K_2/2393745.jpg" ["orange and pink cat ears"]
VM1951:1 2333431 "https://cs.stanford.edu/people/rak248/VG_100K/2333431.jpg" ["the cat is sleepy"]  *********
VM1951:1 2330770 "https://cs.stanford.edu/people/rak248/VG_100K/2330770.jpg" ["head of a cat"]

images.forEach(i=>console.log(i.image_id,i.url));  // image used in paper
images.forEach(i=>console.log(i.image_id,i.url,i.regions[0].text));
VM1953:1 2333432 "https://cs.stanford.edu/people/rak248/VG_100K/2333432.jpg" ["a cat sitting on a window seal"]
VM1953:1 2317478 "https://cs.stanford.edu/people/rak248/VG_100K/2317478.jpg" ["cat paw "]
VM1953:1  285703 "https://cs.stanford.edu/people/rak248/VG_100K/285703.jpg" [" a tile in a floor "]
VM1953:1 2357312 "https://cs.stanford.edu/people/rak248/VG_100K/2357312.jpg" ["cat has his mouth open"] **************
VM1953:1 2344930 "https://cs.stanford.edu/people/rak248/VG_100K/2344930.jpg" ["the reflection of a cat on the water"]
VM1953:1 2379874 "https://cs.stanford.edu/people/rak248/VG_100K_2/2379874.jpg" ["a fluffy cat inside"]
VM1953:1 2363821 "https://cs.stanford.edu/people/rak248/VG_100K/2363821.jpg" ["Adult cat laying on bed. "]
VM1953:1 2332737 "https://cs.stanford.edu/people/rak248/VG_100K/2332737.jpg" ["the head of a cat"]
VM1953:1 2379559 "https://cs.stanford.edu/people/rak248/VG_100K_2/2379559.jpg" ["Cat has pink nose."]
VM1953:1 2365700 "https://cs.stanford.edu/people/rak248/VG_100K/2365700.jpg" ["right eye of cat"]


grep 2357312 data/visual-genome/attributes.json.rows 
#... lots ...

grep 2357312 data/visual-genome/region_graphs.json.rows 

{"regions":[
  {"relationships":[{"synsets":["along.r.01"],"predicate":"ON","relationship_id":4218683,"object_id":3332562,"subject_id":3332563}],
   "region_id":2796477,"width":205,
   "synsets":[{"entity_idx_start":0,"entity_idx_end":3,"entity_name":"cat","synset_name":"cat.n.01"},
              {"entity_idx_start":12,"entity_idx_end":17,"entity_name":"mouth","synset_name":"mouth.n.01"}],
   "height":420,"image_id":2357312,
   "objects":[{"name":"cat","h":370,"object_id":3332562,"synsets":["cat.n.01"],"w":335,"y":40,"x":163},
              {"name":"mouth","h":75,"object_id":3332563,"synsets":["mouth.n.01"],"w":112,"y":103,"x":257}],
   "phrase":"cat has his mouth open","y":3,"x":202
  },
  // ...
]}


Next steps...

BERT code:
  https://github.com/google-research/bert
  https://github.com/huggingface/pytorch-pretrained-BERT

BERT for TPU:
  https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb
  
Also interesting TPU-oriented slides/notebook :  
  http://bit.ly/tpu-dev
  http://bit.ly/keras-tpu

