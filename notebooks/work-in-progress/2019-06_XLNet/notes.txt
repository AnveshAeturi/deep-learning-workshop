## XLNet

*  Paper : XLNet: Generalized Autoregressive Pretraining for Language Understanding
    +  https://arxiv.org/abs/1906.08237
    
###  HN : https://news.ycombinator.com/item?id=20229145

*  This is NOT "just throwing more compute" at the problem.
*  The authors have devised a clever dual-masking-plus-caching mechanism to induce an attention-based model to learn to predict tokens 
   from all possible permutations of the factorization order of all other tokens in the same input sequence. 
*  In expectation, the model learns to gather information from all positions on both sides of each token in order to predict the token.
   +  For example, if the input sequence has four tokens, ["The", "cat", "is", "furry"], 
      in one training step the model will try to predict "is" after seeing "The", then "cat", then "furry". 
   +  In another training step, the model might see "furry" first, then "The", then "cat". 
   +  Note that the original sequence order is always retained, e.g., the model always knows that "furry" is the fourth token.
*  The masking-and-caching algorithm that accomplishes this does not seem trivial to me.
*  The improvements to SOTA performance in a range of tasks are significant -- see tables 2, 3, 4, 5, and 6 in the paper.

*  Primarily from CMU with 5 authors and 1 from Google (with the legend Quoc from Google and Ruslan Salakhutdinov is also excellent)

*  Most of the statements they make regarding orderless autoregression, including statements about the "independence assumption" made by BERT, are misleading at best.
*  See the section "Comparison with BERT" which has a nice worked example that is reasonably understandable 
   without necessarily fully understanding the rest of the paper.
   +   it is obvious that XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals
       -   BERT is only masking 15% of the tokens, so isn't the amount of dependency pairs like 18% higher at most?
           *   That's a small but significant difference, allowing them to improve performance by a small but significant amount.





## Talk outline

*  Features of XLNet :
   +   Transformers (and what do they do)
   +   Tokenisation
   +   MASKing idea (from BERT)
       -   Recent improvements (word-wise)
   +   Sophisticated MASKing / permutations
   +   Training task (remove BERT's sentence-follow task)
       -   Training intensity
   +   Results

*  Extra dimensions
