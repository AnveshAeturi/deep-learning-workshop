## XLNet

*  Paper : XLNet: Generalized Autoregressive Pretraining for Language Understanding
    +  https://arxiv.org/abs/1906.08237

*  Model : https://github.com/zihangdai/xlnet
    -  Original authors : 
    -  Released pretrained model : XLNet-Large, Cased: 24-layer, 1024-hidden, 16-heads
    -  Coming soon : 
          *  We will release an XLNet-Base by the end of June, 2019.
    -  Google Groups :  [XLNet on Google Groups](https://groups.google.com/forum/#!forum/xlnet)
    -  PR for Colab demo notebook : https://github.com/zihangdai/xlnet/pull/47  (Almost 'in')
          *  From https://github.com/aditya-malte/Colab-XLNet-FineTuning
    
*  renatoviolin's repo : https://github.com/renatoviolin/xlnet == XLNet: fine tuning on RTX 2080 GPU - 8 GB
    -  Alternative methods of finetuning XLNet on constrained hardware - which obtained 86.24 F1 on SQuAD2.0 with a 8GB memory GPU.         
    -  With those modifications I could achieve 86,23 F1-Score on the Squad-2.0 dev_set, 
       training for 85000 steps (~ 3 epochs of the full dataset). This training took about 5-6 hours.
    -  Also see : https://github.com/zihangdai/xlnet/issues/64 for announcement/details
       

*  Clarification in https://github.com/zihangdai/xlnet/issues/54 "why not use a partial factorization ?"
   + Assume the original sentence is 12345678, and the permutation is 12367845. 
     The last two tokens, i.e.: 4 and 5, are to be predicted. 
     What we want is: let token 123678 attend to each other 
     (note that according to the paper, i cannot attend to j if i < j, i.e.: 2 only attend to 1, 3 only attend to 12, etc.), 
     let token 4 attend to 123678, and let token 5 attend to 1236784.

   +  I haven't fully understood the source code, but this comment says can attend if i > j or j is non-masked. 
      Combined with the above example, are token 4 and 5 masked? 
      And token 123678 DO attend to each other because j is non-masked?
      If this understanding is correct, please update the formula in the paper accordingly to address this issue. Thanks.

      -  Yes, tokens 123678 have bidirectional attention and they attend to all the other tokens, 
         while tokens 4 and 5 use an auto-regressive factorization conditioned on 123678. 
         This is what we meant in our paper, but I agree that the description was not clear and specific enough. 
         Will fix the paper soon.

*  https://github.com/zihangdai/xlnet/issues/14  "The model did not distinguish all possible factorization order"
   +  I am confused about the factorization order. Looks the model did not distinguish all possible factorization order. ...
      -  Permuting the sequence order is different from permuting the factorization order (See Remark in Sec 2.2). 
         We don't permute the sequence order as in NADE because that would essentially make the model orderless, 
         which is reduced to low-capacity bag-of-words (See related work in Sec 1).
      -  [Loc1(x1), Loc2(x2)] and [Loc2(x2), Loc1(x1)] are the same for a Transformer(-XL), 
         because position encodings are the only thing that defines the sequence order.

    
###  HN : https://news.ycombinator.com/item?id=20229145

*  This is NOT "just throwing more compute" at the problem.
*  The authors have devised a clever dual-masking-plus-caching mechanism to induce an attention-based model to learn to predict tokens 
   from all possible permutations of the factorization order of all other tokens in the same input sequence. 
*  In expectation, the model learns to gather information from all positions on both sides of each token in order to predict the token.
   +  For example, if the input sequence has four tokens, ["The", "cat", "is", "furry"], 
      in one training step the model will try to predict "is" after seeing "The", then "cat", then "furry". 
   +  In another training step, the model might see "furry" first, then "The", then "cat". 
   +  Note that the original sequence order is always retained, e.g., the model always knows that "furry" is the fourth token.
*  The masking-and-caching algorithm that accomplishes this does not seem trivial to me.
*  The improvements to SOTA performance in a range of tasks are significant -- see tables 2, 3, 4, 5, and 6 in the paper.

*  Primarily from CMU with 5 authors and 1 from Google (with the legend Quoc from Google and Ruslan Salakhutdinov is also excellent)

*  Most of the statements they make regarding orderless autoregression, including statements about the "independence assumption" made by BERT, are misleading at best.
*  See the section "Comparison with BERT" which has a nice worked example that is reasonably understandable 
   without necessarily fully understanding the rest of the paper.
   +   it is obvious that XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals
       -   BERT is only masking 15% of the tokens, so isn't the amount of dependency pairs like 18% higher at most?
           *   That's a small but significant difference, allowing them to improve performance by a small but significant amount.

###  Reddit : https://www.reddit.com/r/MachineLearning/comments/c2q5k7/r_xlnet_a_new_pretraining_method_for_nlp_that/

*  Note that XLNet was trained on much larger dataset (10x) for more tokens processed (4x) than BERT, 
   not to mention that the dataset used is much more inherently diverse than before. 
*  Ablation study was performed on the same training dataset (table 6). 
   The result seems to me as if the substantial improvement in this setting is coming mostly from the use of Transformer-XL (i.e. larger context size). 

*  For ablation study in table 6, you can see the advantage of PLM objective function when comparing row 2 & row 3. 
   Row 4 and row 5 also illustrate the effectiveness of memory is far from explaining the gap between BERT and XLNet. 

*  Permutation extracts bidirectional contexts. A left-to-right LM must be much worse for text understanding. 
   Though it might be helpful for text generation, which is beyond the scope of this paper.
   
*  BERT is a bitch to productionize if you can't use GPUs for inference.

*  "BERT neglects dependency between the masked positions" -> each position is predicted independently, 
    i.e. transformer does not see the predicted symbol x_i in the input when it predicts the symbol x_j. Instead it sees x_i = [MASK]
*  "suffers from a pretrain-finetune discrepancy" -> When you fine-tune, the symbol [MASK[ is absent, while the model kinda expects it.

*   So I dug into their released code and found out they set the mask to 'None', i.e. no mask is used during fine-tuning. 
*   Just think of it as a Transformer encoder when used during fine-tuning and evaluation, 
    though it was pretrained in a Transformer-decoder way (get around the standard LM issue by using permutations to get all tokens to see the full context).


## Talk outline

*  Features of XLNet :
   +   Transformers (and what do they do)
   +   Tokenisation
   +   MASKing idea (from BERT)
       -   Recent improvements (word-wise)
           *   newest iteration of BERT (outlined at top of BERT https://github.com/google-research/bert)
   +   Sophisticated MASKing / permutations
   +   Training task (remove BERT's sentence-follow task)
       -   Training intensity  : We train XLNet-Large on 512 TPU v3 chips for 500K steps with an Adam optimizer
   +   Results

*  Extra dimensions


   +   XLNet NER : https://github.com/zihangdai/xlnet/issues/68