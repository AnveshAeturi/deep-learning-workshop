{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WJx1Iv7qRPN"
   },
   "outputs": [],
   "source": [
    "# Upload the IMDB_all_reviews.txt here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "eDRonKjxqVey",
    "outputId": "ee535134-964f-4f50-e2ed-03c71d346088"
   },
   "outputs": [],
   "source": [
    "!wc IMDB*.txt\n",
    "#    25000  6723817 33596339 IMDB_all_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "sop8D99trBQA",
    "outputId": "745d84d4-6e63-4bb4-d263-6123a43d1da2"
   },
   "outputs": [],
   "source": [
    "! head IMDB_all_reviews.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M8wFph-nB13O"
   },
   "outputs": [],
   "source": [
    "# Now split into train / validation (and also lowercase it all)\n",
    "import random\n",
    "\n",
    "reviews_train_len, reviews_valid_len = 0,0\n",
    "\n",
    "with open('IMDB_all_reviews.txt', 'rt') as fin,      \\\n",
    "     open('reviews-train.txt', 'wt') as ftrain, \\\n",
    "     open('reviews-valid.txt', 'wt') as fvalid:\n",
    "  for l in fin:\n",
    "    if random.random()<0.9:\n",
    "      ftrain.write(l.lower())  # No need for +'\\n' - l includes it\n",
    "      reviews_train_len += 1\n",
    "    else:\n",
    "      fvalid.write(l.lower())      \n",
    "      reviews_valid_len += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "uem6DSjYB1yO",
    "outputId": "a33713fd-359d-4399-ea44-444a62c3a9eb"
   },
   "outputs": [],
   "source": [
    "! wc reviews-*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7BSGdzjB1qZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XeY-E7rtsxbM"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import requests, shutil\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YmmjJ8_dxl1T"
   },
   "outputs": [],
   "source": [
    "# ! rm glove.first-100k.6B.50d.txt  # Force download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "colab_type": "code",
    "id": "9DwFRoSSxlut",
    "outputId": "7118db1d-3b53-4af6-f734-3b4ae16c7a5d"
   },
   "outputs": [],
   "source": [
    "# Load the GloVe embedding, along with the words\n",
    "\n",
    "glove_dir = './'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "data_cache = './'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100000: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "if os.path.isfile( glove_100k_50d_path ):\n",
    "  print(\"GloVE available locally\")\n",
    "  ! head -3 {glove_100k_50d_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gcB2wOs30HJE"
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/48677077/how-do-i-create-a-keras-embedding-layer-from-a-pre-trained-word-embedding-datase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "itquBmsx0HEu",
    "outputId": "cb9e31a5-110d-4fc5-b179-4d13a8bf38e5"
   },
   "outputs": [],
   "source": [
    "# Prepare Glove File\n",
    "def readGloveFile(gloveFile):\n",
    "    with open(gloveFile, 'r') as f:\n",
    "        wordToGlove = {}  # map from a token (word) to a Glove embedding vector\n",
    "        wordToIndex = {}  # map from a token to an index\n",
    "        indexToWord = {}  # map from an index to a token \n",
    "\n",
    "        for line in f:\n",
    "            record = line.strip().split()\n",
    "            token = record[0] # take the token (word) from the text line\n",
    "            # associate the Glove embedding vector to a that token (word)\n",
    "            wordToGlove[token] = np.array(record[1:], dtype=np.float64) \n",
    "        \n",
    "        #tokens = sorted(wordToGlove.keys())\n",
    "        #tokens = wordToGlove.keys() # \n",
    "        \n",
    "        token_mask, token_unk = '<MASK>', '<UNK>'\n",
    "        token_list = [token_mask, token_unk,]+list(wordToGlove.keys())\n",
    "        \n",
    "        wordToGlove[token_mask] = np.zeros_like(wordToGlove[token]) \n",
    "        wordToGlove[token_unk]  = np.zeros_like(wordToGlove[token]) \n",
    "        \n",
    "        for idx, tok in enumerate(token_list):\n",
    "            #kerasIdx = idx + 1  # 0 is reserved for masking in Keras (see above)\n",
    "            #kerasIdx = idx\n",
    "            #wordToIndex[tok] = kerasIdx # associate an index to a token (word)\n",
    "            #indexToWord[kerasIdx] = tok # associate a word to a token (word). \n",
    "            wordToIndex[tok] = idx # associate an index to a token (word)\n",
    "            indexToWord[idx] = tok # associate a word to a token (word). \n",
    "            # Note: inverse of dictionary above\n",
    "\n",
    "    return wordToIndex, indexToWord, wordToGlove, token_list\n",
    "\n",
    "wordToIndex, indexToWord, wordToGlove, token_list = readGloveFile(glove_100k_50d_path)\n",
    "[ indexToWord[i] for i in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eo7NNbW7aiYb"
   },
   "outputs": [],
   "source": [
    "# Create Pretrained Keras Embedding Layer\n",
    "def createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, isTrainable):\n",
    "    #vocabLen = len(wordToIndex) + 1  # adding 1 to account for masking\n",
    "    vocabLen = len(wordToIndex)\n",
    "    embDim = next(iter(wordToGlove.values())).shape[0]  # works with any glove dimensions (e.g. 50)\n",
    "\n",
    "    embeddingMatrix = np.zeros((vocabLen, embDim))    # initialize with zeros\n",
    "    for word, index in wordToIndex.items():\n",
    "        embeddingMatrix[index, :] = wordToGlove[word] # create embedding: word index to Glove word embedding\n",
    "\n",
    "    embeddingLayer = keras.layers.Embedding(vocabLen, embDim, \n",
    "                                     weights=[embeddingMatrix], \n",
    "                                     mask_zero=True,  # zero embedding for zero_padding\n",
    "                                     trainable=isTrainable)\n",
    "    return embeddingLayer, embDim\n",
    "  \n",
    "pretrainedEmbeddingLayer, embedding_dim = createPretrainedEmbeddingLayer(wordToGlove, wordToIndex, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yiIkvhVFadKA"
   },
   "outputs": [],
   "source": [
    "# usage\n",
    "#model = Sequential()\n",
    "#model.add(pretrainedEmbeddingLayer)\n",
    "# or\n",
    "#model.add(Embedding(max_features, 128, mask_zero = True))  # zero embedding for zero_padding\n",
    "#model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ms069Ow_0HAa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1oeVu3YUrM33"
   },
   "outputs": [],
   "source": [
    "## Terrible documentation :\n",
    "# https://www.tensorflow.org/guide/datasets#consuming_text_data\n",
    "\n",
    "## Much better documentation :\n",
    "# https://cs230-stanford.github.io/tensorflow-input-data.html#introduction-to-tfdata-with-a-text-example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HAwd9p4xlrY"
   },
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/api_docs/python/tf/contrib/lookup/index_table_from_tensor\n",
    "\n",
    "mapping_strings = tf.constant(token_list)\n",
    "embedding_mapping = tf.contrib.lookup.index_table_from_tensor(\n",
    "    mapping=mapping_strings, num_oov_buckets=0, default_value=wordToIndex['<UNK>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ve7yOmLHsxXN"
   },
   "outputs": [],
   "source": [
    "reviews_train = tf.data.TextLineDataset(\"reviews-train.txt\")\n",
    "reviews_valid = tf.data.TextLineDataset(\"reviews-valid.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kq8YurX8tp6C"
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "batch_size  = 128\n",
    "\n",
    "def preprocess_line(line):\n",
    "  line_data = tf.string_split([line], delimiter='|').values\n",
    "  \n",
    "  label = tf.string_to_number( line_data[0], out_type=tf.int32)\n",
    "  txt = tf.string_split([ line_data[1] ], \n",
    "                        delimiter=' ', \n",
    "                        skip_empty=True).values  # lower-case conversion done above\n",
    "    \n",
    "  # so now txt is an tf vector of strings - each a word\n",
    "   \n",
    "  # If we wanted to do a char-wise analysis, we could use the code below\n",
    "  #   *if* we could split the original line_data[1] into \n",
    "  #   a vector of single character-length strings...\n",
    "\n",
    "  \n",
    "  # Other way to split into words ... \n",
    "  # https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/text_to_word_sequence\n",
    "  #txt = tf.keras.preprocessing.text.text_to_word_sequence( line_data[1] )\n",
    "\n",
    "  txt_ids = embedding_mapping.lookup(txt)\n",
    "  label_onehot = tf.one_hot(label, depth=num_classes, axis=-1)\n",
    "  \n",
    "  # This gives us :\n",
    "  #   reviews[0] as a review tensor with variable length; and \n",
    "  #   reviews[1] as label\n",
    "  return txt_ids, label_onehot\n",
    "\n",
    "def batch_padded(ds, is_training=False, buffer_size=100, batch_size=batch_size):\n",
    "  if is_training:\n",
    "    ds = ds.shuffle(buffer_size=buffer_size)\n",
    "  ds = ds.repeat(100)  # \"Forever\"\n",
    "  ds = ds.map(preprocess_line, num_parallel_calls=4)\n",
    "  ds = ds.padded_batch(batch_size, \n",
    "           padded_shapes=(tf.TensorShape([None]), tf.TensorShape([num_classes])), \n",
    "           #padding_values=(0,0)  # Defaults to 0 padding, which is <MASK> which is fine\n",
    "          )\n",
    "  \n",
    "  ds = ds.prefetch(1)  # Makes it run async (prefetch 1 batch ahead)\n",
    "  return ds  \n",
    "\n",
    "reviews_train_ds = batch_padded(reviews_train, is_training=True)\n",
    "reviews_valid_ds = batch_padded(reviews_valid, is_training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1139
    },
    "colab_type": "code",
    "id": "LhzkDu70sxQs",
    "outputId": "6a04699d-ab1c-46b5-a5dc-0e2d55b2610c"
   },
   "outputs": [],
   "source": [
    "#train_iterator = reviews_train_ds.make_one_shot_iterator()  # Does not work : DS includes a lookup table\n",
    "train_iterator = reviews_train_ds.make_initializable_iterator()\n",
    "train_next_batch = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "neJacOIVsxT6",
    "outputId": "16788b82-a48c-401a-b09b-67e86ea0070b"
   },
   "outputs": [],
   "source": [
    "tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1139
    },
    "colab_type": "code",
    "id": "LhzkDu70sxQs",
    "outputId": "6a04699d-ab1c-46b5-a5dc-0e2d55b2610c"
   },
   "outputs": [],
   "source": [
    "# This just proves that the dataset iterator can read a review,\n",
    "#   and then convert it back from the indices to words\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run( tf.tables_initializer() )\n",
    "  sess.run( train_iterator.initializer )\n",
    "  for i in range(1):\n",
    "    sentence, label = sess.run(train_next_batch)\n",
    "    print(label[0], ' :: ', (' '.join([indexToWord[i] for i in sentence[0] ])[:200]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GkJwU31gsxK0"
   },
   "outputs": [],
   "source": [
    "# http://ruder.io/text-classification-tensorflow-estimators/ # TF. Estimators => not relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "# Props to :\n",
    "\n",
    "# https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py\n",
    "#   Just for the model (since the built-in imdb dataset makes it 'too easy')\n",
    "\n",
    "# https://stackoverflow.com/questions/46135499/how-to-properly-combine-tensorflows-dataset-api-and-keras\n",
    "# https://gist.github.com/datlife/abfe263803691a8864b7a2d4f87c4ab8\n",
    "#   For the dataset direct to keras example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "source": [
    "## Now for the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "inputs, targets = train_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "source": [
    "### Sequential style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "#model = keras.Sequential()\n",
    "#model.add(keras.layers.Embedding(max_features, 128))\n",
    "#model.add(pretrainedEmbeddingLayer)\n",
    "#model.add(keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(keras.layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "source": [
    "### Functional style "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "model_input = keras.layers.Input(tensor=inputs)\n",
    "#x = keras.layers.Embedding(max_features, 128)(model_input)\n",
    "x = pretrainedEmbeddingLayer(model_input)\n",
    "x = keras.layers.LSTM(128, dropout=0.2, recurrent_dropout=0.2)(x)\n",
    "model_output = keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "train_model = keras.models.Model(inputs=model_input, outputs=model_output)\n",
    "print('Model built')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=2e-3, decay=1e-5)\n",
    "\n",
    "train_model.compile(optimizer=optimizer,\n",
    "                    loss='categorical_crossentropy',\n",
    "                    target_tensors=[targets],\n",
    "                    metrics=['accuracy'],\n",
    "                   )\n",
    "\n",
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "epochs=1  # Should get to acc:0.81 (each epoch ~5mins)\n",
    "steps_per_epoch=reviews_train_len // batch_size  \n",
    "\n",
    "print('Train...')\n",
    "with keras.backend.get_session().as_default() as sess:\n",
    "  sess.run( tf.tables_initializer() )\n",
    "  sess.run( train_iterator.initializer )\n",
    "train_model.fit(epochs=epochs, steps_per_epoch=steps_per_epoch, verbose=1)\n",
    "print('Training ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cannot predict directly from this dataset-trained model : need to save and reload as a regular model...\n",
    "#train_model.predict({inputs:batch_predict})  # , batch_size=1\n",
    "#train_model.predict(batch_predict)  # , batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model.save('IMDB-saved-model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = keras.models.load_model('IMDB-saved-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def to_idx(s):\n",
    "    # Remove punctuation characters except for the apostrophe\n",
    "    #  https://docs.python.org/3/library/stdtypes.html?highlight=maketrans#str.maketrans\n",
    "    translator = str.maketrans('', '', string.punctuation.replace(\"'\", ''))\n",
    "    tokens = s.translate(translator).lower().split()\n",
    "    return [wordToIndex.get(t, wordToIndex['<UNK>']) for t in tokens]\n",
    "    #return np.array(\n",
    "    #           [1] + [word_index[t] + \n",
    "    #           index_offset if t in word_index else 2 for t in tokens])    \n",
    "to_idx('this movie was excellent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_predict = np.array([ \n",
    "    to_idx('this movie was excellent'),\n",
    "    to_idx('i hated the movie'),\n",
    "] )\n",
    "test_model.predict(batch_predict)  # , batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ABDj8iIhsqbj"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=3,\n",
    "          validation_data=(x_test, y_test))\n",
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfadfasdfasdfs\n",
    "\n",
    "# FIT / TRAIN model\n",
    "\n",
    "NumEpochs = 10\n",
    "BatchSize = 512\n",
    "\n",
    "# OPTIMIZERS\n",
    "#model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, \n",
    "                    validation_data=(x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(\"_\"*100)\n",
    "print(\"Test Loss and Accuracy\")\n",
    "print(\"results \", results)\n",
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IMBD-txt-to-Dataset",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}